---
title: "Eric_Hirsch_621_Assignment_4"
subtitle: "Predicting Insurance Claims" 
author: "Eric Hirsch"
date: "4/7/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =  FALSE, message = FALSE)
```

```{r, include=FALSE}
library(tidyverse)
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(patchwork)
library(gridExtra)
library(ggsci)
library(caret)
library(pROC)
library(car)
```
```{r}

df <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 3\\crime-training-data_modified.csv")
dfEval <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 3\\crime-training-data_modified.csv")

df2 <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 4\\insurance_training_data.csv")

```

We examine records of car insurance customers to build two predictive models: one for whether the customer would have crashed, and second, the $ amount paid for the crash.  

The main issue in the dataset are outliers.  Without transformation, the distribution of residuals is not normal, and there are too many influential points to create reliable models.

### 1. Data Exploration


#### A. Summary Statistics

We first examine the data. The dataset consists of 8161 observations and 26 variables (including two target variables, TARGET_FLAG and TARGET_AMT). 14 of the predictor variables are numeric. Approximately 27% of customers had an accident - the rest did not. TARGET_AMT appears to be highly skewed.

```{r}

library(readr)
df2$OLDCLAIM <- parse_number(df2$OLDCLAIM)
df2$INCOME <- parse_number(df2$INCOME)
df2$HOME_VAL <- parse_number(df2$HOME_VAL)
df2$BLUEBOOK <- parse_number(df2$BLUEBOOK)

df2Crashed <- df2 %>%
  dplyr::filter(TARGET_FLAG==1.0)

summary(df2)
str(df2)

```


#### B. Distributions

##### 1. Boxplota


```{r}
library(psych)
library(patchwork)

a <- EHSummarize_SingleColumn_Boxplots(df2)
grid.arrange(grobs=a[c(2:16)], ncol=4)

```

##### 2. Histograms

```{r}

a <-EHSummarize_SingleColumn_Histograms(df2)
grid.arrange(grobs=a[c(2:16)], ncol=4)
```
Many of the variables are highly skewed, particularly TARGET_AMT. The level of outliers is very high.


#### C. Multicollinearity

The chart below shows multicollinearity for numerical variables only. There are no surprises here - older people tend not to have children at home, income and home value are related, etc.  Multicollinearity does not present offhand as a major issue.

```{r}

df2Num <- df2 %>%
  dplyr::select_if(is.numeric)

EHExplore_Multicollinearity(df2Num, run_all=FALSE)

```

### 2. Data Preparation

#### A. Create Dummy Variables

We create dummy variables from the character variables in the database.  

```{r}
library(tidytable)

df3 <- EHPrepare_CreateDummies(df2)

df4 <- df3 %>%
   #dplyr::filter(row_number() %% 5 == 1)  %>%
  dplyr::select(-INDEX)

```

#### B. Address Missing Values

We consider the missing values. Over 20% of the records have missing values.

We disregard missing values in character columns because these NAs were isolated out in their own columns when we dummified the data. We convert the 0s in INCOME and HOME_VAL to NA since 0 is implausible.  We create flags to track the NAs for the columns with the most significant NAs - INCOME, HOME_VAL, CAR_AGE, and YOJ.  Finally we use MICE to populate the missing values.

```{r, include=FALSE}
library(MASS)
g <- EHSummarize_MissingValues(df4)

df5 <- df4 %>%
  mutate(INCOME=ifelse(INCOME==0, NA, INCOME), HOME_VAL=ifelse(HOME_VAL==0, NA, HOME_VAL))

df6 <- df5 %>%
  mutate(YOJ_NA = ifelse(is.na(YOJ),0,1), INCOME_NA = ifelse(is.na(INCOME),0,1), CAR_AGE_NA = ifelse(is.na(CAR_AGE),0,1), HOME_VAL_NA = ifelse(is.na(HOME_VAL),0,1))

library(mice)
mice1 <- mice(df6,m=5,seed=2)
df7 <- complete(mice1,1)

```

```{r}

print(g[1])

```


####. C. Perform Transformations

We perform log and other transformations, as well as add an interaction term, to the analysis.  These transformations are based on an examination of the distributions of the indepenent variables.  They include:

ageSquared\
yojSquared\
income_log\
homeval_log\
travtime_log\
bluebook_log\
carage_log\
oldclaim_log\
clm_freq_log\
mvr_pts_log\
tif_log\
kidsdriv_log\
homekids_log\
inter (interaction term = KIDSDRIV*AGE\

```{r}

df8 <- df7 %>%
  mutate(ageSquared = AGE^2, yojSquared = YOJ^2, income_log = log(INCOME+1), homeval_log = log(HOME_VAL+1),travtime_log = log(TRAVTIME+1), bluebook_log = log(BLUEBOOK+1), carage_log = log(CAR_AGE+1), oldclaim_log = log(OLDCLAIM+1), clm_freq_log = log(CLM_FREQ+1), mvr_pts_log = log(MVR_PTS+1), tif_log = log(TIF+1), kidsdriv_log=log(KIDSDRIV+1), homekids_log=log(HOMEKIDS+1), inter = KIDSDRIV*AGE)
```


### 2. Predict TARGET_FLAG

```{r}

df8Flag <- df8 %>%
  dplyr::select(-TARGET_AMT)

dfNum_Flag <- df2Num %>%
  dplyr::select(-TARGET_AMT, -INDEX)


df8AMT <- df8 %>%
  dplyr::select(-TARGET_FLAG)

dfNum_AMT <- df2Num %>%
  dplyr::select(-TARGET_FLAG, -INDEX)

```

#### A. Explore relationships

We can see from the boxplots run on the original numeric variables against TARGET_FLAG that the correlations are quite low. 

```{r}



ty <- EHExplore_OneContinuousAndOneCategoricalColumn_Boxplots(dfNum_Flag, "TARGET_FLAG")

grid.arrange(grobs=ty[c(2:9)], ncol=2, nrow=4)
grid.arrange(grobs=ty[c(10:15)], ncol=2, nrow=4)

```


#### B. Create Models

##### Create Model 1 - the base model with the original numeric variables.

```{r}

EHModel_Regression_Logistic(dfNum_Flag, "TARGET_FLAG", splitRatio = 1, xseed=10)
```

```{r, include=FALSE}

EHModel_Regression_Logistic_Iterations(dfNum_Flag, "TARGET_FLAG")

```

Most of the predictors are significant.  This may in part be due to the fact that there are over 8,000 predictions.  The model has an AIC of 6702. Almost 20% of the records are missing so the model is not necessarily reliable.

We run the model 100 times at a 80/20 split.  The base model has an accuracy of .748, an AIC of 5266 and an AUC of .716.

##### Create Model 2 - a model with missings addressed and all of the transformed and added variables.

```{r}

EHModel_Regression_Logistic(df8Flag, "TARGET_FLAG", splitRatio = 1, xseed =10)

```

```{r, include=FALSE}

EHModel_Regression_Logistic_Iterations(df8Flag, "TARGET_FLAG")

```

In the "kitchen sink" model many of the predictors are not significant.  This model risks overprediction. AIC has increased to 7269.

We run the model 100 times at a 80/20 split.  The kitchen sink model has an accuracy of .79, an AIC of 5723 and an AUC of .815.  Despite possible overprediction, this model offers significant improvement.

##### Create Model 3 - Use backward elmination to choose the best model:

We use backward elimination to achieve a better fit and lower AIC.

```{r}

df7b <- df8Flag %>%
  dplyr::select(-JOB_Home.Maker, -CAR_TYPE_Panel.Truck, -EDUCATION_Bachelors, -JOB_Doctor, -JOB_Clerical, -tif_log, -mvr_pts_log, -carage_log, -clm_freq_log, -CLM_FREQ, -bluebook_log, -yojSquared)

df7c <- df7b %>%
  dplyr::select(-RED_CAR_no, -YOJ, -JOB_z_Blue.Collar, -EDUCATION_.High.School, -kidsdriv_log, -MVR_PTS)

df7d <- df7c %>%
  dplyr::select(-KIDSDRIV, -travtime_log, -MSTATUS_Yes, -HOME_VAL)


df7e <- df7d %>%
  dplyr::select(-REVOKED_Yes, -JOB_Student, -income_log, -ageSquared, -AGE, -CAR_AGE, -CAR_TYPE_Van)

df7f <- df7e %>%
  dplyr::select(-homekids_log, -HOMEKIDS, -homeval_log, -INCOME_NA, -YOJ_NA, -CAR_AGE_NA, -JOB_, -EDUCATION_z_High.School, -EDUCATION_Masters, -JOB_Lawyer)

```

```{r}

EHModel_Regression_Logistic(df7f, "TARGET_FLAG", splitRatio = 1, xseed = 10)

```

```{r, include=FALSE}

EHModel_Regression_Logistic_Iterations(df7f, "TARGET_FLAG")
```

The refined model improves AIC dramatically to 777.

We run the model 100 times at a 80/20 split.  The refined model has an accuracy of .77, an AIC of 601 and an AUC of .78.  Despite lower AIC, this model does not predict the data as well.

#### 4. Select model

Below is a table of results:
```{r}

tab <- matrix(c(.748, .79, .77, 5266, 5723, 601, .716, .815, .78), ncol=3, byrow=TRUE)
colnames(tab) <- c('Base Model','Kitchen Sink Model','Refined Model')
rownames(tab) <- c('Accuracy','AIC','AUC')
tab <- as.table(tab)

knitr::kable(tab)

```

Despite the apparent superior predictability of the second model, we choose the third. This model has a lower AIC and is more interpretable and coherent than the second model.  

### 5. Predict TARGET_AMT

#### A.Explore Relationships

```{r}

ws <- EHExplore_TwoContinuousColumns_Scatterplots(dfNum_AMT, "TARGET_AMT")
grid.arrange(grobs=ws[c(2:14)], ncol=4)

```

#### B. Create models

##### Create Model 1 - the base model with the original numeric variables.

```{r}
#1. Base - outliers and non-normal distribution and heteroskedastic

dfNum_AMT <- na.omit(dfNum_AMT)

dfc7 <- dfNum_AMT

aa <- EHModel_Regression_StandardLM(dfNum_AMT, "TARGET_AMT", tests=FALSE)
hist(aa$residuals)

```

The base model shows a number of issues with the residuals including heteroskedastcity and particularly non-normal residuals.  We cannot use this model without some transformation.

##### Create Model 2 - a model with missings addressed and all of the transformed and added variables.

```{r}

#2. Remove main outliers - just results in new outliers and distribution is the same and still heteroskedastic

dfc71 <- dfc7 %>%
  dplyr::filter(rownames(dfc7) != 66, rownames(dfc7) != 26, rownames(dfc7) != 183, rownames(dfc7) != 570, rownames(dfc7) != 260, rownames(dfc7) != 147)

bb <- EHModel_Regression_StandardLM(dfc71, "TARGET_AMT", tests = FALSE)
#hist(bb[[1]]$residuals)

#3. Log of y - still nonormal (but closer to normal) with outliers, but not homoskedatsic

dfc72 <- dfc7
dfc72$TARGET_AMT <- log(dfc72$TARGET_AMT+1)
cc<- EHModel_Regression_StandardLM(dfc72, "TARGET_AMT", tests = FALSE)
#hist(cc[[1]]$residuals)

```
```{r}


# Add log terms - residuals closer to normal but not (not skewed but heavy tailed), and outliers remain

dfc73 <- dfc72 %>%
  mutate(ageSquared = AGE^2, yojSquared = YOJ^2, income_log = log(INCOME+1), homeval_log = log(HOME_VAL+1),travtime_log = log(TRAVTIME+1), bluebook_log = log(BLUEBOOK+1), carage_log = log(CAR_AGE+1), oldclaim_log = log(OLDCLAIM+1), clm_freq_log = log(CLM_FREQ+1), mvr_pts_log = log(MVR_PTS+1), tif_log = log(TIF+1), kidsdriv_log=log(KIDSDRIV+1), homekids_log=log(HOMEKIDS+1), inter = KIDSDRIV*AGE)

dfc73 <- na.omit(dfc73)
zz <- EHModel_Regression_StandardLM(dfc73, "TARGET_AMT", splitRatio = 1, tests = FALSE)
print(hist(dfc73$TARGET_AMT))
#hist(zz[[1]]$residuals)

```

```{r}

qwe <- EHModel_Regression_StandardLM(dfc73, "TARGET_AMT", tests = FALSE)
qwr <- EHModel_Regression_Robust(dfc73, "TARGET_AMT")

hist(qwe$residuals)
hist(qwr[[1]]$residuals)
hist(qwe[[4]])
hist(qwr[[4]])

qwe[[2]]
qwr[[2]]
qwe[[3]]
qwr[[3]]

```



```{r}
#aa1 <- EHModel_Regression_Standard_Iterations(aa[[1]]$model, "TARGET_AMT")
#aa2 <- EHModel_Regression_Robust_Iterations(aa[[1]]$model, "TARGET_AMT")

```
```{r}
#aa3 <- EHModel_Regression_Standard_Iterations(cc[[1]]$model, "TARGET_AMT")
#aa4 <- EHModel_Regression_Robust_Iterations(cc[[1]]$model, "TARGET_AMT")
```

```{r}
#xd5 <- EHModel_Regression_Standard_Iterations(zz[[1]]$model, "TARGET_AMT")
#xd6 <- EHModel_Regression_Robust_Iterations(zz[[1]]$model, "TARGET_AMT")

```


```{r}

# above plus massive outlier elimination - no progress


#9.5 and 7.5 work

dfc74 <- dfc73 %>%
  dplyr::filter(TARGET_AMT<9 & TARGET_AMT>8)

#xb <- lm(TARGET_AMT~., dfc74)
  
uu <- EHModel_Regression_StandardLM(dfc74, "TARGET_AMT")
print(hist(dfc74$TARGET_AMT))
#print(hist(uu[[1]]$residuals))
```

```{r}
#second try

asd <- EHModel_Regression_StandardLM(dfc7, "TARGET_AMT", splitRatio = 1, xseed=101, tests = FALSE)

xresidual <- asd$residuals


dfMissing2 <- dfMissing %>%
  dplyr::select(-TARGET_AMT)

dfNew <- cbind(dfc7, xresidual)
dfMissing2 <-cbind(dfMissing2, xresidual)
  

#hist(asd[[1]]$residuals)

dfNew2 <- dfNew %>%
  dplyr::filter(abs(xresidual)<4000) %>%
  dplyr::select(-xresidual)

asd2 <- EHModel_Regression_StandardLM(dfNew2, "TARGET_AMT", xseed = 101, tests = FALSE)

#hist(asd2[[1]]$residuals)


#outliers:

dfNew_Out <- dfNew %>%
  dplyr::filter(abs(xresidual)>4000)

#hist(dfNew_Out$xresidual)

#EHSummarize_StandardPlots(dfNew_Out, "TARGET_AMT")

dfNew_Int <- dfNew %>%
  dplyr::mutate(resid_flag = ifelse(abs(xresidual)>4000,1,0)) %>%
  dplyr::select(-xresidual)

dfMissing3 <- dfMissing2 %>%
  dplyr::mutate(resid_flag = ifelse(abs(xresidual)>4000,1,0)) %>%
  dplyr::select(-xresidual)

#wq <- EHExplore_Interactions_Scatterplots(dfNew_Int, "TARGET_AMT", "resid_flag")
#grid.arrange(grobs = wq[c(1:12)], ncol=3, nrow=4)
#grid.arrange(grobs = wq[c(13:24)], ncol=3, nrow=4)
#grid.arrange(grobs = wq[c(25:36)], ncol=3, nrow=4)
#grid.arrange(grobs = wq[c(37:39)], ncol=3, nrow=1)

summary(dfNew2)
str(dfNew2)

we <- EHExplore_OneContinuousAndOneCategoricalColumn_Boxplots(dfNew_Int, "resid_flag")
grid.arrange(grobs = we[c(1,6,7,9,22,35)], ncol=3, nrow=2)

EHExplore_TwoCategoricalColumns_Barcharts(dfMissing3, "resid_flag")


mean(dfNew2$INCOME)
mean(dfNew_Out$INCOME)
```

```{r}
EHModel_Regression_Robust(zz[[1]]$model, "TARGET_AMT")

```

```{r}

#robust regression using log model

robust1 <- rlm(TARGET_AMT ~ ., data=zz[[1]]$model)
robust2 <- rlm(TARGET_AMT ~ ., data=aa[[1]]$model)
robust3 <- rlm(TARGET_AMT ~ ., data=cc[[1]]$model)

ols1 <- lm(TARGET_AMT ~ ., data=zz[[1]]$model)
ols2 <- lm(TARGET_AMT~., data=aa[[1]]$model )
ols3 <- lm(TARGET_AMT~., data=cc[[1]]$model )


summary(robust1)

#find residual standard error of ols model
summary(ols1)$sigma

#find residual standard error of robust model
summary(robust1)$sigma

summary(ols2)$sigma

#find residual standard error of robust model
summary(robust2)$sigma

summary(ols3)$sigma

#find residual standard error of robust model
summary(robust3)$sigma

#Lower is better


```


### 10. Conclusion

We examined 466 records of town statistics to create a predictive model of whether crime rates were above the median or not. We used a logistic regression to do this, testing our models on an 80/20 split 100 times and taking the average accuracy and AIC.

Several enhancements to the model increased accuracy and lowered AIC.  First, some predictors were transformed with the log or square to improve fit. Second, dummy variables were introduced to capture the fact that highly industrial areas appeared to operate by a different logic than mixed use areas.  interaction terms to model this phenomenon did not improve the model.  The final model 93% accurate.

