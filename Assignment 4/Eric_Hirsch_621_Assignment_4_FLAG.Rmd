---
title: "Eric_Hirsch_621_Assignment_4"
subtitle: "Predicting Insurance Claims" 
author: "Eric Hirsch"
date: "4/7/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =  FALSE, message = FALSE)
```

```{r}
library(tidyverse)
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(patchwork)
library(gridExtra)
library(ggsci)
library(caret)
library(pROC)
library(car)
```
```{r}

df <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 3\\crime-training-data_modified.csv")
dfEval <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 3\\crime-training-data_modified.csv")

df2 <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 4\\insurance_training_data.csv")

```

We examine records of car insurance customers to build two predictive models: one for whether the customer would have crashed, and second, the $ amount paid for the crash.  

The main issue in the dataset are outliers.  Without transformation, the distribution of residuals is not normal, and there are two many influential points to create reliable models.

### 1. Data Exploration


#### A. Summary Statistics

We first examine the data. The dataset consists of 8161 observations and 26 variables (including two target variables, TARGET_FLAG and TARGET_AMT). 14 of the predictor variables are numeric. Approximately 27% of customers had an accident - the rest did not. TARGET_AMT appears to be highly skewed.

```{r}

library(readr)
df2$OLDCLAIM <- parse_number(df2$OLDCLAIM)
df2$INCOME <- parse_number(df2$INCOME)
df2$HOME_VAL <- parse_number(df2$HOME_VAL)
df2$BLUEBOOK <- parse_number(df2$BLUEBOOK)

df2Crashed <- df2 %>%
  dplyr::filter(TARGET_FLAG==1.0)

summary(df2)
str(df2)

```

#### B. Distributions

```{r}
library(psych)
library(patchwork)

a <- EHSummarize_SingleColumn_Boxplots(df2)
grid.arrange(grobs=a[c(2:16)], ncol=4)

a <-EHSummarize_SingleColumn_Histograms(df2)
grid.arrange(grobs=a[c(2:16)], ncol=4)
```
Many of the variables are highly skewed, particularly TARGET_AMT. The level of outliers is very high.


#### C. Multicollinearity

The chart below shows multicollinearity for numerical variables only. There are no surprises here - older people tend not to have children at home, income and home value are related, etc.  Multicollinearity does not present offhand as a major issue.

```{r}

df2Num <- df2 %>%
  dplyr::select_if(is.numeric)

EHExplore_Multicollinearity(df2Num, run_all=FALSE)

```

### 2. Data Preparation

#### A. Create Dummy Variables

We create dummy variables from the character variables in the database.  

```{r}
library(tidytable)

df3 <- EHPrepare_CreateDummies(df2)

df4 <- df3 %>%
   dplyr::filter(row_number() %% 10 == 1)  %>%
  dplyr::select(-INDEX)

```

#### B. Address Missing Values

We consider the missing values. We disregard missing values in character columns because these NAs were isolated out in their own columns when we dummified the data. We convert the 0s in INCOME and HOME_VAL to NA since 0 is implausible.  We create flags to track the NAs for the columns with the most significant NAs - INCOME, HOME_VAL, CAR_AGE, and YOJ.  Finally we use MICE to populate the missing values.

```{r}
library(MASS)
EHSummarize_MissingValues(df4)

df5 <- df4 %>%
  mutate(INCOME=ifelse(INCOME==0, NA, INCOME), HOME_VAL=ifelse(HOME_VAL==0, NA, HOME_VAL))

df6 <- df5 %>%
  mutate(YOJ_NA = ifelse(is.na(YOJ),0,1), INCOME_NA = ifelse(is.na(INCOME),0,1), CAR_AGE_NA = ifelse(is.na(CAR_AGE),0,1), HOME_VAL_NA = ifelse(is.na(HOME_VAL),0,1))

library(mice)
mice1 <- mice(df6,m=5,seed=2)
df7 <- complete(mice1,1)

```

####. C. Perform Transformations

We perform log and other transformations, as well as add an interaction term, to the analysis.  These transformations are based on an examination of the distributions of the indepenent variables.  They include:

ageSquared
yojSquared
income_log
homeval_log
travtime_log 
bluebook_log 
carage_log
oldclaim_log
clm_freq_log
mvr_pts_log
tif_log
kidsdriv_log
homekids_log
inter (interaction term = KIDSDRIV*AGE

```{r}

df8 <- df7 %>%
  mutate(ageSquared = AGE^2, yojSquared = YOJ^2, income_log = log(INCOME+1), homeval_log = log(HOME_VAL+1),travtime_log = log(TRAVTIME+1), bluebook_log = log(BLUEBOOK+1), carage_log = log(CAR_AGE+1), oldclaim_log = log(OLDCLAIM+1), clm_freq_log = log(CLM_FREQ+1), mvr_pts_log = log(MVR_PTS+1), tif_log = log(TIF+1), kidsdriv_log=log(KIDSDRIV+1), homekids_log=log(HOMEKIDS+1), inter = KIDSDRIV*AGE)
```


### 2. Predicting TARGET_FLAG

```{r}

df8Flag <- df8 %>%
  dplyr::select(-TARGET_AMT)

dfNum_Flag <- df2Num %>%
  dplyr::select(-TARGET_AMT, -INDEX)

```

#### A. Explore relationships

We can see from the boxplots run on the original numeric variables against TARGET_FLAG that the correlations are quite low. 

```{r}



ty <- EHExplore_OneContinuousAndOneCategoricalColumn_Boxplots(dfNum_Flag, "TARGET_FLAG")

grid.arrange(grobs=ty[c(2:9)], ncol=2, nrow=4)
grid.arrange(grobs=ty[c(10:15)], ncol=2, nrow=4)

```
#### B. 

##### Create Model 1 - the base model with the original numeric variables.

```{r}

EHModel_Regression_Logistic_Iterations(dfNum_Flag, "TARGET_FLAG")

```

The base model has an accuracy of .748, an AIC of 5270 and an AUC of .716.

##### Create Model 2 - the new model with all of the transformed and added variables.
```{r}

EHModel_Regression_Logistic_Iterations(df8Flag, "TARGET_FLAG")

```

The "kitchen sink" model offers a little improvement: Accuracy .752, AIC 654 and AUC .75.

```{r}


df5b <- df5 %>%
  dplyr::select(-CAR_TYPE_Panel.Truck, -EDUCATION_Bachelors, -JOB_Home.Maker, -JOB_Doctor, -HOMEKIDS, -AGE, -YOJ, -OLDCLAIM, -EDUCATION_.High.School, -JOB_Clerical, -JOB_Student, -JOB_z_Blue.Collar, -RED_CAR_no)

df5c <- df5b %>%
  dplyr::select(-CAR_TYPE_Van, -MVR_PTS, -MSTATUS_Yes, -EDUCATION_z_High.School)

#df5d <- df5c %>%
#  dplyr::select(-KIDSDRIV, -travtime_log, -MSTATUS_Yes, -HOME_VAL)

EHModel_Regression_Logistic(df5c, "TARGET_FLAG", splitRatio = 1, xseed = 10)

```


```{r}

EHModel_Regression_Logistic_Iterations(df5c, "TARGET_FLAG")

```


```{r}


df6 <- df5 %>%
  dplyr::select(TARGET_FLAG, HOMEKIDS, OLDCLAIM, CLM_FREQ, MVR_PTS, PARENT1_Yes, MSTATUS_Yes, CAR_TYPE_z_SUV, REVOKED_Yes, URBANICITY_z_Highly.Rural..Rural)

EHModel_Regression_Logistic(df6, "TARGET_FLAG")

```



### 2. Data Preparation

#### A. Interaction terms


#### B. Transformations

When the predictor variable is normally distributed but with different variance for the two values of y, we may try a quadratic function of x.  rm may be considered a reasonable candidate for this.

For skewed distributions, we may include both x and log(x).  nox, age, dis and lstat may be candidates for this. 

Therefore we add the following transformations:

rmSquared, nox_log, age_log, dis_log, lstat_log


### 3. Build Models

#### A. Base Model

We begin with the base model (all original variables from the original dataset).  


```{r}
EHModel_Regression_Logistic(df, "target", splitRatio = 1)

```

In the base model, only 4 of the 12 predictors are not significant. AIC is 218. Through backward elimination we remove rm, lstat, chas and indus and arrive at a model with an AIC of 215.

Now we split the dataset 80/20 and perform 100 training iterations to test model predictive power.  

```{r include=FALSE}

dfx <- df %>%
  dplyr::select(-rm, -lstat, -chas, -indus)

acc = list()
AIC = list()

for (i in 1:100)
{
  q <- EHModel_Regression_Logistic(dfx, "target")
  acc[i]=q[2]
  AIC[i]=q[3]
}

```
```{r}
accv <- unlist(acc)
aveq <- mean(accv)

aicv <- unlist(AIC)
aicq <- mean(aicv)

print(paste("Accuracy: ", aveq))
print(paste("AIC: ", aicq))


```

The base model alone is an excellent predictor of crime rate. Accuracy is 91%. AIC of the smaller model is 171 (AIC will drop with fewer observations.)

__*The final results for this model, averaging 100 rounds on an 80/20 split are 91% accuracy and an AIC of 215 for the full model.*__

#### B. Enhanced Model with Dummies and Transformations

Now we try a model with all of our dummy variables from above and as well as our interactions:

```{r}

dfInt2 <- dfInt %>%
  mutate(rmSquared = rm^2,nox_log = log(nox), age_log = log(age),dis_log = log(dis),lstat_log = log(lstat))

dfEval <- dfEval %>%
  mutate(rmSquared = rm^2,nox_log = log(nox), age_log = log(age),dis_log = log(dis),lstat_log = log(lstat))

df7 <- df5 %>%
  mutate(ageSquared = AGE^2, yojSquared = YOJ^2, income_log = log(INCOME+1), homeval_log = log(HOME_VAL+1),travtime_log = log(TRAVTIME+1), bluebook_log = log(BLUEBOOK+1), carage_log = log(CAR_AGE+1), oldclaim_log = log(OLDCLAIM+1), clm_freq_log = log(CLM_FREQ+1), mvr_pts_log = log(MVR_PTS+1), tif_log = log(TIF+1), kidsdriv_log=log(KIDSDRIV+1), homekids_log=log(HOMEKIDS+1), inter = KIDSDRIV*AGE)
  

EHModel_Regression_Logistic_Iterations(df7, "TARGET_FLAG")
```


```{r}

df7b <- df7 %>%
  dplyr::select(-JOB_Home.Maker, -CAR_TYPE_Panel.Truck, -EDUCATION_Bachelors, -JOB_Doctor, -JOB_Clerical, -tif_log, -mvr_pts_log, -carage_log, -clm_freq_log, -CLM_FREQ, -bluebook_log, -yojSquared)

df7c <- df7b %>%
  dplyr::select(-RED_CAR_no, -YOJ, -JOB_z_Blue.Collar, -EDUCATION_.High.School, -kidsdriv_log, -MVR_PTS)

df7d <- df7c %>%
  dplyr::select(-KIDSDRIV, -travtime_log, -MSTATUS_Yes, -HOME_VAL)

df7e <- df7d %>%
  dplyr::select(-REVOKED_Yes, -JOB_Student, -income_log, -ageSquared, -AGE, -CAR_AGE)


df7e <- df7d %>%
  dplyr::select(-REVOKED_Yes, -JOB_Student, -income_log, -ageSquared, -AGE, -CAR_AGE, -CAR_TYPE_Van)

df7f <- df7e %>%
  dplyr::select(-homekids_log, -HOMEKIDS)

EHModel_Regression_Logistic(df7f, "TARGET_FLAG", splitRatio = 1, xseed = 10)

```

```{r}

EHModel_Regression_Logistic_Iterations(df7f, "TARGET_FLAG", numOfIterations = 200)

```


```{r}

df8 <- df7 %>%
  dplyr::select(TARGET_FLAG, AGE, TRAVTIME, PARENT1_Yes, CAR_TYPE_Pickup, CAR_TYPE_Sports.Car, CAR_TYPE_z_SUV, JOB_Lawyer, JOB_Manager, JOB_Student, AGE, ageSquared, URBANICITY_z_Highly.Rural..Rural, homeval_log, BLUEBOOK, TIF, SEX_z_F)

EHModel_Regression_Logistic_Iterations(df8, "TARGET_FLAG")
```



```{r}

df9 <- df8 %>%
  dplyr::select(-BLUEBOOK, -TIF, -SEX_z_F)

EHModel_Regression_Logistic_Iterations(df9, "TARGET_FLAG")

```


```{r include=FALSE}

z <- EHModel_Regression_Logistic(dfInt2, "target", splitRatio = 1 )
```

Despite the larger number of variables, many of which are not significant, our AIC improves substantially to 178.  Through backward elimination we remove a number of variables and reduce the AIC to 163.  The new model can be seen below. Despite the fact that the tax variable does not seem significant, we leave it in, as taking it out increases AIC substantially.

```{r}

dfInt5 <- dfInt2 %>%
  dplyr::select(-NoxOverPoint8, -nox, -radOver10, -ZnOver0, -lstat_log, -lstat, -zn, -indus, -MedvBelow50, -chas, -medv)

dfEval <- dfEval %>%
    dplyr::select(-NoxOverPoint8, -nox, -radOver10, -ZnOver0, -lstat_log, -lstat, -zn, -indus, -MedvBelow50, -chas, -medv)

EHModel_Regression_Logistic(dfInt5, "target", splitRatio = 1)

```


We split the dataset 80/20 and perform 100 training iterations to test model predictive power.

```{r include=FALSE}
acc = list()
AIC = list()

for (i in 1:100)
{
  q <- EHModel_Regression_Logistic(dfInt5, "target")
  acc[i]=q[2]
  AIC[i]=q[3]
}

```
```{r}
accv <- unlist(acc)
aveq <- mean(accv)

aicv <- unlist(AIC)
aicq <- mean(aicv)

print(paste("Accuracy: ", aveq))
print(paste("AIC: ", aicq))


```


The model does a better job predicting outcomes (accuracy = .94) than the base model, and the AIC for the smaller model falls from 171 to 134. 

__*The final results for this model, averaging 100 rounds on an 80/20 split are 93% accuracy and an AIC of 163 for the full model.*__

#### C. Enhanced Model with Interaction Terms

For this model we add interaction terms in addition to dummies and transformations.  We choose the following interactions as they seem the most promising:

inter_z_rm = ZnOver0$*$rm\
inter_age_indus = IndusOver16$*$age\
inter_rad_lstat = radOver10$*$lstat\
inter_pt_rad = ptOver14$*$rad

```{r}

dfInt6 <- dfInt2 %>%
  mutate(inter_z_rm = ZnOver0*rm) %>%
  mutate(inter_age_indus = IndusOver16*age) %>%
  mutate(inter_rad_lstat = radOver10*lstat) %>% 
  mutate(inter_pt_rad = ptOver14*rad) 

```

```{r}

EHModel_Regression_Logistic(dfInt6, "target", splitRatio = 1)

```
None of the interaction terms are significant. AIC for the larger model climbs to 185, and the for the smaller it rises as well. Accuracy falls below 93%.  We will therefore reject the use of interaction terms.


```{r include=FALSE}
acc = list()
AIC = list()

for (i in 1:100)
{
  q <- EHModel_Regression_Logistic(dfInt6, "target")
  acc[i]=q[2]
  AIC[i]=q[3]
}

```
```{r}
accv <- unlist(acc)
aveq <- mean(accv)

aicv <- unlist(AIC)
aicq <- mean(aicv)

print(paste("Accuracy: ", aveq))
print(paste("AIC: ", aicq))


```


### 4. Select Model

We choose model B as the model with both best accuracy and lowest AIC. First we run some diagnostics on the model.

```{r}

  m1 <- glm(target ~ .,
                        data = dfInt5,
                        family = "binomial")
  print(m1)
  plot(m1)
  mmps(m1)
```

Observations 338 and 280 appear to be outliers and possibly influential points (especially 280).  Since they are only two points, we will eliminate them to see their impact on accuracy and AIC:


```{r include=FALSE}


dfInt9 <- dfInt5 %>%
  dplyr::filter(rownames(dfInt5) !="338" & rownames(dfInt5) !="280")

acc = list()
AIC = list()

for (i in 1:100)
{
  q <- EHModel_Regression_Logistic(dfInt9, "target")
  acc[i]=q[2]
  AIC[i]=q[3]
}

```
```{r}
accv <- unlist(acc)
aveq <- mean(accv)

aicv <- unlist(AIC)
aicq <- mean(aicv)

print(paste("Accuracy: ", aveq))
print(paste("AIC: ", aicq))


```

This improves both accuracy and AIC and so (with the risk of overfitting) we accept the change.

Our final model is as follows. Results may differ since there as an 80/20 split - however, what is shown below is typical.

```{r}

EHModel_Regression_Logistic(dfInt9, "target", xseed = 31)

```

The final step is to make predictions on the evaluation set:

```{r}

makePredictions <- function(m)
{
predictions <- as.data.frame(predict(m,newdata=dfEval, type="response"))
write_csv(predictions, "D:\\RStudio\\CUNY_621\\predictionsCrime.csv")
}

a <- makePredictions(m1)

head(a)

```
### 5. Conclusion

We examined 466 records of town statistics to create a predictive model of whether crime rates were above the median or not. We used a logistic regression to do this, testing our models on an 80/20 split 100 times and taking the average accuracy and AIC.

Several enhancements to the model increased accuracy and lowered AIC.  First, some predictors were transformed with the log or square to improve fit. Second, dummy variables were introduced to capture the fact that highly industrial areas appeared to operate by a different logic than mixed use areas.  interaction terms to model this phenomenon did not improve the model.  The final model 93% accurate.

