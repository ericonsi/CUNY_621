---
title: "Eric_Hirsch_621_Assignment_3"
subtitle: "Predicting Town Crime Rates" 
author: "Eric Hirsch"
date: "4/7/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning =  FALSE, message = FALSE)
```

```{r}
library(tidyverse)
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(patchwork)
library(gridExtra)
library(ggsci)
library(caret)
library(pROC)
library(car)
library(Metrics)
```
```{r}

df <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 3\\crime-training-data_modified.csv")
dfEval <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 3\\crime-training-data_modified.csv")

df2 <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 4\\insurance_training_data.csv")

```




### 1. Data Exploration


#### A. Summary Statistics

We first examine the data. The dataset consists of 466 observations and 13 variables, all numeric.  Two are binary, including the target. There are no missing values.  The target appears to be relatively balanced (which makes sense, as it is an indicator of being above or below the median.)   

```{r}

library(readr)
df2$OLDCLAIM <- parse_number(df2$OLDCLAIM)
df2$INCOME <- parse_number(df2$INCOME)
df2$HOME_VAL <- parse_number(df2$HOME_VAL)
df2$BLUEBOOK <- parse_number(df2$BLUEBOOK)

```

```{r}


library(tidytable)

cols <- c('PARENT1', 'MSTATUS', 'SEX', 'CAR_TYPE', 'RED_CAR', 'REVOKED', 'JOB', 'CAR_USE', 'EDUCATION', 'URBANICITY')

df3 <- df2 %>%
  get_dummies.(cols,  drop_first = TRUE) %>%
  dplyr::select(-cols)

df4 <- data.frame(df3) 

df2Crashed <- df4 %>%
  dplyr::filter(TARGET_FLAG==1.0)

  dfc5 <- df2Crashed %>%
#dplyr::filter(row_number() %% 3 == 1)  %>%
  dplyr::select(-INDEX, -TARGET_FLAG)


summary(dfc5)
str(dfc5)

```

```{r}
library(psych)

dfc6_MissingFlags <- dfc5 %>%
  mutate(YOJ_NA = ifelse(is.na(YOJ),1,0), INCOME_NA = ifelse(is.na(INCOME)|INCOME==0,1,0), CAR_AGE_NA = ifelse(is.na(CAR_AGE),1,0), HOME_VAL_NA = ifelse(is.na(HOME_VAL)|HOME_VAL==0,1,0))

#a <- EHSummarize_StandardPlots(dfc6, "TARGET_AMT", type="scatter")

```

What do you see? goes here


#### B. Multicollinearity


```{r}
#EHExplore_Multicollinearity(df2, run_all=FALSE)

```

```{r}
library(MASS)
EHSummarize_MissingValues(dfc5)
```
```{r}
dfc6 <- dfc5 %>%
  mutate(YOJ_NA = ifelse(is.na(YOJ),0,1), INCOME_NA = ifelse(is.na(INCOME),0,1), CAR_AGE_NA = ifelse(is.na(CAR_AGE),0,1), HOME_VAL_NA = ifelse(is.na(HOME_VAL),0,1))

dfMissing <- dfc6 %>%
  dplyr::select(TARGET_AMT, YOJ_NA, INCOME_NA, CAR_AGE_NA, HOME_VAL_NA)

m1 <- lm(TARGET_AMT~., dfMissing)
summary(m1)

boxplot(dfc6$TARGET_AMT, dfc6$CAR_AGE_NA)

#dfc67 <- EHPrepare_MissingValues_Imputation(dfc5, "TARGET_AMT")

dfM1 <- dfc6 %>%
  dplyr::select(-CAR_AGE, -INCOME, -HOME_VAL)

#EHExplore_Interactions_Scatterplots(dfM1, "TARGET_AMT", "YOJ_NA")

```


```{r}

#EHExplore_Multicollinearity(df5, run_all = TRUE)

```
```{r}

#Missing
library(mice)

dfc51 <- dfc5 %>%
  mutate(INCOME=ifelse(INCOME==0, NA, INCOME), HOME_VAL=ifelse(HOME_VAL==0, NA, HOME_VAL))

mice1 <- mice(dfc51,m=5,seed=2)
dfc7 <- complete(mice1,1)
```
```{r}
#1. Base - outliers and nonormal distributio nand heteroskedastic

aa <- EHModel_Regression_StandardLM(dfc7, "TARGET_AMT")
hist(aa[[1]]$residuals)


```
```{r}
#xd <- EHModel_Regression_Robust_Iterations(dfc7, "TARGET_AMT")


```


```{r}
# same thing with mean imputation instead


dfc7a <- EHPrepare_MissingValues_Imputation(dfc51, "TARGET_AMT")
aq <- EHModel_Regression_StandardLM(dfc7a, "TARGET_AMT")
hist(aq[[1]]$residuals)
```
```{r}

#2. Remove main outliers - just resuts in new outliers and distribution is the same and still heteroskedastic

dfc71 <- dfc7 %>%
  dplyr::filter(rownames(dfc7) != 66, rownames(dfc7) != 26, rownames(dfc7) != 183, rownames(dfc7) != 570, rownames(dfc7) != 260, rownames(dfc7) != 147)

bb <- EHModel_Regression_StandardLM(dfc71, "TARGET_AMT")
hist(bb[[1]]$residuals)

#3. Log of y - still nonormal (but closer to normal) with outliers, but not homoskedatsic

dfc72 <- dfc7
dfc72$TARGET_AMT <- log(dfc72$TARGET_AMT)
cc<- EHModel_Regression_StandardLM(dfc72, "TARGET_AMT")
hist(cc[[1]]$residuals)

```
```{r}


# Add log terms - residuals closer to normal but not (not skewed but heavy tailed), and outliers remain

dfc73 <- dfc72 %>%
  mutate(ageSquared = AGE^2, yojSquared = YOJ^2, income_log = log(INCOME+1), homeval_log = log(HOME_VAL+1),travtime_log = log(TRAVTIME+1), bluebook_log = log(BLUEBOOK+1), carage_log = log(CAR_AGE+1), oldclaim_log = log(OLDCLAIM+1), clm_freq_log = log(CLM_FREQ+1), mvr_pts_log = log(MVR_PTS+1), tif_log = log(TIF+1), kidsdriv_log=log(KIDSDRIV+1), homekids_log=log(HOMEKIDS+1), inter = KIDSDRIV*AGE)

dfc73 <- na.omit(dfc73)
zz <- EHModel_Regression_StandardLM(dfc73, "TARGET_AMT", splitRatio = 1)
print(hist(dfc73$TARGET_AMT))
hist(zz[[1]]$residuals)

```

```{r}

qwe <- EHModel_Regression_StandardLM(dfc7, "TARGET_AMT")
qwr <- EHModel_Regression_Robust(dfc7, "TARGET_AMT")

hist(qwe[[1]]$residuals)
hist(qwr[[1]]$residuals)
hist(qwe[[4]])
hist(qwr[[4]])

qwe[[2]]
qwr[[2]]
qwe[[3]]
qwr[[3]]

```



```{r}
#aa1 <- EHModel_Regression_Standard_Iterations(aa[[1]]$model, "TARGET_AMT")
#aa2 <- EHModel_Regression_Robust_Iterations(aa[[1]]$model, "TARGET_AMT")

```
```{r}
#aa3 <- EHModel_Regression_Standard_Iterations(cc[[1]]$model, "TARGET_AMT")
#aa4 <- EHModel_Regression_Robust_Iterations(cc[[1]]$model, "TARGET_AMT")
```

```{r}
#xd5 <- EHModel_Regression_Standard_Iterations(zz[[1]]$model, "TARGET_AMT")
#xd6 <- EHModel_Regression_Robust_Iterations(zz[[1]]$model, "TARGET_AMT")

```


```{r}

# above plus massive outlier elimination - no progress


#9.5 and 7.5 work

dfc74 <- dfc73 %>%
  dplyr::filter(TARGET_AMT<9 & TARGET_AMT>8)

#xb <- lm(TARGET_AMT~., dfc74)
  
uu <- EHModel_Regression_StandardLM(dfc74, "TARGET_AMT")
print(hist(dfc74$TARGET_AMT))
print(hist(uu[[1]]$residuals))
```

```{r}
#second try

asd <- EHModel_Regression_StandardLM(dfc7, "TARGET_AMT", splitRatio = 1, xseed=101)

xresidual <- asd[[1]]$residuals


dfMissing2 <- dfMissing %>%
  dplyr::select(-TARGET_AMT)

dfNew <- cbind(dfc7, xresidual)
dfMissing2 <-cbind(dfMissing2, xresidual)
  

hist(asd[[1]]$residuals)

dfNew2 <- dfNew %>%
  dplyr::filter(abs(xresidual)<4000) %>%
  dplyr::select(-xresidual)

asd2 <- EHModel_Regression_StandardLM(dfNew2, "TARGET_AMT", xseed = 101)

hist(asd2[[1]]$residuals)


#outliers:

dfNew_Out <- dfNew %>%
  dplyr::filter(abs(xresidual)>4000)

hist(dfNew_Out$xresidual)

#EHSummarize_StandardPlots(dfNew_Out, "TARGET_AMT")

dfNew_Int <- dfNew %>%
  dplyr::mutate(resid_flag = ifelse(abs(xresidual)>4000,1,0)) %>%
  dplyr::select(-xresidual)

dfMissing3 <- dfMissing2 %>%
  dplyr::mutate(resid_flag = ifelse(abs(xresidual)>4000,1,0)) %>%
  dplyr::select(-xresidual)

#wq <- EHExplore_Interactions_Scatterplots(dfNew_Int, "TARGET_AMT", "resid_flag")
#grid.arrange(grobs = wq[c(1:12)], ncol=3, nrow=4)
#grid.arrange(grobs = wq[c(13:24)], ncol=3, nrow=4)
#grid.arrange(grobs = wq[c(25:36)], ncol=3, nrow=4)
#grid.arrange(grobs = wq[c(37:39)], ncol=3, nrow=1)

summary(dfNew2)
str(dfNew2)

we <- EHExplore_OneContinuousAndOneCategoricalColumn_Boxplots(dfNew_Int, "resid_flag")
grid.arrange(grobs = we[c(1,6,7,9,22,35)], ncol=3, nrow=2)

EHExplore_TwoCategoricalColumns_Barcharts(dfMissing3, "resid_flag")


mean(dfNew2$INCOME)
mean(dfNew_Out$INCOME)
```

```{r}
EHModel_Regression_Robust(zz[[1]]$model, "TARGET_AMT")

```

```{r}

#robust regression using log model

robust1 <- rlm(TARGET_AMT ~ ., data=zz[[1]]$model)
robust2 <- rlm(TARGET_AMT ~ ., data=aa[[1]]$model)
robust3 <- rlm(TARGET_AMT ~ ., data=cc[[1]]$model)

ols1 <- lm(TARGET_AMT ~ ., data=zz[[1]]$model)
ols2 <- lm(TARGET_AMT~., data=aa[[1]]$model )
ols3 <- lm(TARGET_AMT~., data=cc[[1]]$model )


summary(robust1)

#find residual standard error of ols model
summary(ols1)$sigma

#find residual standard error of robust model
summary(robust1)$sigma

summary(ols2)$sigma

#find residual standard error of robust model
summary(robust2)$sigma

summary(ols3)$sigma

#find residual standard error of robust model
summary(robust3)$sigma

#Lower is better


```


### 2. Data Preparation

#### A. Interaction terms
  These are the dummy variables we've chosen, based on the histograms above:

    TaxOver600
    radOver10
    ptOver14
    lstatOver12
    IndusOver16
    ZnOver0
    NoxOverPoint8
    MedvBelow50

The following are just some of the possible interactions we discover affecting the dataset:

```{r}

#dfInt <- df %>%
#    mutate(TaxOver600 = ifelse(tax>=600,1,0)) 

#dfEval <- dfEval %>%
    #mutate(TaxOver600 = ifelse(tax>=600,1,0))

#a <- EHExplore_Interactions_Scatterplots(dfInt, "target", "ZnOver0")
#b <- EHExplore_Interactions_Scatterplots(dfInt, "target", "IndusOver16")


#x <- c(a[2], a[5], a[9], b[4], b[6], c[11], d[10], e[8], e[2], e[9])

#grid.arrange(grobs=x, ncol=3, nrow=4)

```

From these plots, we might draw a number of conclusions.  First, 

These interactions may or may not prove useful.  We don't want to overfit the data, so it may be enough to simply include the dummy variables.

#### B. Transformations

When the predictor variable is normally distributed but with different variance for the two values of y, we may try a quadratic function of x.  rm may be considered a reasonable candidate for this.

For skewed distributions, we may include both x and log(x).  nox, age, dis and lstat may be candidates for this. 

Therefore we add the following transformations:

rmSquared, nox_log, age_log, dis_log, lstat_log


### 3. Build Models

#### A. Base Model

We begin with the base model (all original variables from the original dataset).  


```{r}
#EHModel_Regression_Logistic(df, "target", splitRatio = 1)

```

In the base model, only 4 of the 12 predictors are not significant. AIC is 218. Through backward elimination we remove rm, lstat, chas and indus and arrive at a model with an AIC of 215.

Now we split the dataset 80/20 and perform 100 training iterations to test model predictive power.

#### C. Enhanced Model with Interaction Terms



### 4. Select Model

We choose model B as the model with both best accuracy and lowest AIC. First we run some diagnostics on the model.

### 5. Conclusion

We examined 466 records of town statistics to create a predictive model of whether crime rates were above the median or not. We used a logistic regression to do this, testing our models on an 80/20 split 100 times and taking the average accuracy and AIC.

Several enhancements to the model increased accuracy and lowered AIC.  First, some predictors were transformed with the log or square to improve fit. Second, dummy variables were introduced to capture the fact that highly industrial areas appeared to operate by a different logic than mixed use areas.  interaction terms to model this phenomenon did not improve the model.  The final model 93% accurate.

