---
title: "Eric_Hirsch_621_Assignmrnt_2"
author: "Eric Hirsch"
date: "3/20/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
#tinytex::install_tinytex()

```


```{r, warnings = FALSE, messages=FALSE}
library(caret)
library(pROC)
library(tidyverse)
```

Read the data

```{r}

dfB <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 2\\classification-output-data.csv")
dfB <- dfB %>%
  dplyr::select(class, scored.class, scored.probability)

```

## A. Review the raw confusion matrix - 

```{r}

dfB1 <- dfB %>%
  dplyr::select(-scored.probability)

table(dfB1)

dfTable <- as.data.frame(table(dfB1))

```

The raw confusion matrix shows us the actual values as rows and the scored values as columns. The upper left-lower right diagonal contains correct predictions, while the opposite diagonal contains incorrect predictions.

## B. Calculate Prediciton Metrics

We write a function to return accuracy (correct predictions over all predictions)

```{r}

TableToDf <- function (df)
{
  df1 = as.data.frame(table(df))
  li <- vector(mode = "list", length = 4)
  names(li) <- c("TP", "TN", "FP", "FN")
  
  li$TP <- df1$Freq[4]
  li$TN <- df1$Freq[1]
  li$FP <- df1$Freq[3]
  li$FN <- df1$Freq[2]
  
  return(li)

}

x <- TableToDf(dfB1)
```

### 1. Accuracy Function

```{r}

Accuracy <- function(df) {
  
li <- TableToDf(df)
Accuracy <- (li$TP + li$TN)/(li$TP + li$TN + li$FP + li$FN)

return (Accuracy)

}

print(Accuracy(dfB1))
```

### 2. Classification Error Rate Function

Classification Error Rate (incorrect predictions over all predicitons:
```{r}

ClassificationErrorRate <- function(df) {

li <- TableToDf(df)
Error <- (li$FP + li$FN)/(li$TP + li$TN + li$FP + li$FN)

return (Error)
  
  
}
print(ClassificationErrorRate(dfB1))
```

Accuracy + error rate = 1

```{r}

print(as.numeric(ClassificationErrorRate(dfB1)) + as.numeric(Accuracy(dfB1)))

```

### 3. Precision Function (True positives/All who tested positive):

```{r}

Precision <- function(df) {
  
li <- TableToDf(df)
Precision <- (li$TP)/(li$TP + li$FP)

return(Precision)
  
}
print(Precision(dfB1))
```
### 4. Sensitivity/Recall Function (True positives/All who are positive):

```{r}

Sensitivity <- function(df) {
  
li <- TableToDf(df)

  
  Sensitivity <- (li$TP)/(li$TP + li$FN)

  

return(Sensitivity)
  
}
print(Sensitivity(dfB1))

```

### 5. Specificity Function (True negatives/All who are negative):
```{r}

Specificity <- function(df) {
  
li <- TableToDf(df)
Specificity <- (li$TN)/(li$TN + li$FP)

return(Specificity)
  
}
print(Specificity(dfB1))

```
### 6. F1 score Function:
```{r}

F1 <- function(df) {


p <- Precision(dfB1)
s <- Sensitivity(dfB1)

F1 <- (2*p*s)/(p+s)

return(F1)
  
}

print(F1(dfB1))

```

## C. Bounds on the F1 score

Precision (P) and Sensitivity (S) themselves are always <= 1 because their denominators are a simple sum which include the numerators.  Therefore, P$*$S < P and P$*$S < S. Therefore, 2PS < P + S.

## D. ROC curve Function

```{r}

ROC <- function(df) {
  
list1 <- list()
  
dfNew <- data.frame(class = numeric(),
 scored.class = numeric())

dfFinal <- data.frame(Specificity = numeric(),
 Sensitivity = numeric(), Area = numeric(), Width=numeric())

spec_prev = 0
sens_prev = 0
  
for (i in 1:100) {
  for (j in 1:length(df$class))
  {
    
  dfNew[j, 1] = df$class[j]
  dfNew[j, 2]  <- ifelse(df$scored.probability[j] > i/100, 1, 0)
  }
  
  sens <- Sensitivity(dfNew)
  spec <- 1 - Specificity(dfNew)
  
  width = spec_prev - spec
  ave_sens = (sens + sens_prev)/sens
  
  dfFinal[i,1] <- spec
  dfFinal[i,2] <- sens
  dfFinal[i,3] <- sens*width
  dfFinal[i,4] <- width
  
  spec_prev = spec
  sens_prev = sens
}

dfFinal <- na.omit(dfFinal)

AUC <- sum(dfFinal$Area)/sum(dfFinal$Width)
print (paste("AUC:", AUC))

g <- ggplot(dfFinal, aes(x=Specificity, y=Sensitivity)) +
             geom_line()

x <- list(g, dfFinal$Area)

return (x)
}

```


## E. Provide all metrics

```{r, warning=FALSE, message=FALSE, echo=FALSE}

My_Metrics <- list()
My_Metrics[1] <- Accuracy(dfB1)
My_Metrics[2] <- ClassificationErrorRate(dfB1)
My_Metrics[3] <- Precision(dfB1)
My_Metrics[4] <- Sensitivity(dfB1)
My_Metrics[5] <- Specificity(dfB1)
My_Metrics[6] <- F1(dfB1)

print(paste("Accuracy: ", My_Metrics[[1]]))
print(paste("Classification Error: ", My_Metrics[[2]]))
print(paste("Precision: ", My_Metrics[[3]]))
print(paste("Sensitivity: ", My_Metrics[[4]]))
print(paste("Specificity:", My_Metrics[[5]]))
print(paste("F1:", My_Metrics[[6]]))

x1 <- ROC(dfB)
print(x1[[1]])

```

## F. The caret package provides statistics that match ours.

```{r}

dfB1$class <- as.factor(dfB1$class)
dfB1$scored.class <- as.factor(dfB1$scored.class)

a <- confusionMatrix(data = dfB1$scored.class, reference = dfB1$class, positive="1")

q <- a$byClass["Sensitivity"]

Caret_Metrics <- list()
Caret_Metrics[1] <- a$overall["Accuracy"]
Caret_Metrics[2] <- 1 - as.numeric(Caret_Metrics[[1]])
Caret_Metrics[3] <- a$byClass["Precision"]
Caret_Metrics[4] <- a$byClass["Sensitivity"]
Caret_Metrics[5] <- a$byClass["Specificity"]
Caret_Metrics[6] <- a$byClass["F1"]

Metric <- c("Accuracy", "ClassificationErrorRate", "Precision", "Sensitivity", "Specificity", "F1")

dfX <- as.data.frame(cbind(Metric, My_Metrics, Caret_Metrics))
knitr::kable(dfX)

print(a)


```
## G. The pROC ROC curve

The pROC package can be used to create an ROC curve.  This curve matches our own.  The AUC scores are slightly different (.84 vs .85)

```{r}
roc(class ~ scored.probability, dfB)

roc1 <- roc(dfB$class,
            dfB$scored.probability, plot=TRUE)
```