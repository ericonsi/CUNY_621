---
title: "Eric_Hirsch_621_Assignment_3"
subtitle: "Predicting Wine Cases Bought" 
author: "Eric Hirsch"
date: "4/7/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =  FALSE, message = FALSE)
```

```{r}
library(tidyverse)
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(patchwork)
library(gridExtra)
library(ggsci)
library(caret)
library(pROC)
library(car)
library(psych)
```
```{r}

df <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 5\\wine-training-data.csv", fileEncoding="UTF-8-BOM")
#df <- read.csv("C:\\Users\\eric.hirsch\\Desktop\\RStudio\\CUNY_621\\Assignment 5\\wine-training-data.csv", fileEncoding="UTF-8-BOM")

#dfEval <- read.csv("D:\\RStudio\\CUNY_621\\Assignment 3\\crime-training-data_modified.csv")

df <- df %>%
  dplyr::select(-INDEX)

```

### 1. Data Exploration


#### A. Summary Statistics

We are modeling a data set containing information on approximately 12,000 commercially available wines. The variables are mostly related to the chemical properties of the wine being sold. The response variable, "TARGET", is the number of sample cases of wine that were purchased by wine distribution companies after sampling a wine. 

We first examine the data. The dataset consists of 12795 observations and 15 variables, all numeric.  Some of the variables are count data, including the dependent varable "TARGET."  The target variable has a minimum of 0, a maximum of 8 and a median of 3.  Because of the small number of counts, this dataset is likely to be best modeled with a poisson or negative binomial count model.   

The mssing values should also be noted.  In the case of STARS, e.g., 26% of the values (3359) are missing.  We will need to make some decisions about these missing values.

```{r}

summary(df)
str(df)

```

Now we examine distributions:


```{r}

dfSmall <- df %>%
 dplyr::filter(row_number() %% 20 == 1)

a <- EHSummarize_StandardPlots(dfSmall, "TARGET", type="scatter")
```

Looking at boxplots, histograms, and scatterplots against the target variable for each variable in the dataset, we see some areas of interest. First, the target variable appears to be normally distributed for wines that had at least one purchase - but a large percentage of have zero purchases.  This suggests the need for a zero-inflated model.  Second, most of the variables exhibit the same pattern - a high peak at the mean of the distribution with long tails on either end.  Finally, AcidIndex, LabelAppeal and Stars have, by far, the highest direct correlations with TARGET. 

#### B. Multicollinearity

Looked at from a strictly pairwise view, there is very little mutlicollinearity in this database. This is, frankly, very surprising given pairings like "FreeSulfurDixide" and "TotalSulfurDioxide", and "FixedAcidity", "VolatileAcidity" and "Ph". If I were in a workplace I would definitely explore these findings more with a chemist to understand them better and verify them.

```{r}
EHExplore_Multicollinearity(df, run_all=FALSE)


```

Whether we choose a poisson or negative binomial model will depend on the residuals of our poisson model.  Once we account for missing values, add columns and trasnform columns, our model will change. It is therefore a good exercise to run a poisson model at this point to get someinsight into the data.
 
```{r}

library(faraway)
modp <- glm(TARGET ~., family=poisson, df)  
summary(modp)
#halfnorm(residuals(modp)) 

```
We can see that The residual deviance falls far short of the degrees of freedom, suggesting signifcant overdispersion.  However, improvement in the model may help to account for more of the variance.  Also, we are missing almost half the data due to missing values.

### 2. Data Preparation

#### A. Address Missing Values

We consider the missing values. 

```{r}

g <- EHSummarize_MissingValues(df)
print (g[[1]])
print(g[[3]])
```

Over half of the records have missing values. Missings are confined to 8 variables.  Interestingly, there is not a lot of overlap among the missings.  

With so many missing values, it's prudent to create flags to track what is missing. 


```{r}

df6 <- df %>%
  mutate(STARS_NA = ifelse(is.na(STARS),1,0), Sulphates_NA = ifelse(is.na(Sulphates),1,0), TotalSulpherDioxide_NA = ifelse(is.na(TotalSulfurDioxide),1,0), Alcohol_NA = ifelse(is.na(Alcohol),1,0), FreeSulfurDioxide_NA = ifelse(is.na(FreeSulfurDioxide),1,0), Chlorides_NA = ifelse(is.na(Chlorides),1,0), ResidualSugar_NA = ifelse(is.na(ResidualSugar),1,0), pH_NA = ifelse(is.na(pH),1,0))

```
We investigate STARS_NA further:

```{r}

ww <- EHExplore_OneContinuousAndOneCategoricalColumn_Boxplots(df6, "STARS_NA")
grid.arrange(grobs=ww[c(1:8)], ncol=2)
grid.arrange(grobs=ww[c(9:15)], ncol=2)


```
We can see how strongly Stars_na correlates with TARGET.  Wines that have no stars tend to get bought at a far lower rate than those that do.  The variance is also much higher.

We can see outliers at the top of the first boxplot - this represents two wines that were bought at the highest quantity despite having missing stars.  We remove them from the analysis since they are clearly idiosyncratic anomalies. 

```{r}

df6a <- df6 %>%
  dplyr::filter(STARS_NA == 0 | TARGET != 8)
  
```

We impute values for the NAs.  Because the most significant of them are in the STARS category, a count category from 1 to 5, we will simply impute the median.

```{r, include=FALSE}
library(mice)
#mice1 <- mice(df6a,m=5,seed=2)
#df7 <- complete(mice1,1) 

df7 <- EHPrepare_MissingValues_Imputation(df6a, impute="median")
```

```{r}

df7Small <- df7 %>%
 dplyr::filter(row_number() %% 20 == 1)

#write.csv(df7Small, "D:\\RStudio\\CUNY_621\\Assignment 5\\dataset.csv")

```

#### B. Transformations and interaction terms

We do a log transformation of AcidIndex (logAcidIndex) to account for the skew.  We also add two interaction terms through analysis were discovered to be possible candidates - inter_Starsna_labelappeal and inter_stars_acidindex.

```{r}
library(MASS)

df7Inter <- df7 %>%
  mutate(inter_Starsna_labelappeal = STARS_NA*LabelAppeal, inter_stars_acidindex = STARS*AcidIndex, logAcidIndex = log(AcidIndex))

df7a <- EHPrepare_ScaleAllButTarget(df7Inter, "TARGET")
```

### 3. Build Models

#### A. Base Model

We begin with a base model - OLS on all of the variables. We create two versions - one with all variables retained, and one with backward elimination to minimize the AIC.


```{r}

df7a <- na.omit(df7a)

OLS_all <- EHModel_Regression_StandardLM(df7a, "TARGET", splitRatio=1, tests = FALSE, xseed=10, xstepAIC = FALSE, returnLM=TRUE, vif=FALSE)

OLS_MinimizeAIC <- EHModel_Regression_StandardLM(df7a, "TARGET", splitRatio=1, tests = FALSE, xseed=10, returnLM=TRUE, vif = FALSE)

library(broom)
OLS_all_results <- tidy(OLS_all)
OLS_MinimizeAIC_Results <- tidy(OLS_MinimizeAIC)
#write.csv(aa, "C:\\Users\\eric.hirsch\\Desktop\\RStudio\\CUNY_621\\Assignment 5\\Standard.csv")


```
Many of the varaibles are significant, including both our interaction terms. The AIC of the inclusive model is 40929 while the AIC maxed model is 40914.

#### B. Poisson Model

```{r}
library(faraway)
modp <- glm(TARGET ~., family=poisson, df7a)  
summary(modp)
#halfnorm(residuals(modp)) 

Poisson_results <- tidy(modp)
#write.csv(bb, "C:\\Users\\eric.hirsch\\Desktop\\RStudio\\CUNY_621\\Assignment 5\\poisson.csv")
```

The deviance and residuals have come much closer together, close enough that we may use the poisson distribution rather than the negative binomial.  In a work situation we might want to verify that the imputed missing values have not overly artificially reduced the variance, but we will not do that here.

The AIC has increased in this model.  The poisson model by itself is not a better model than the base model.

#### C. Zero Inflated model

Over 20% of the wines had zero cases bought, second only to 4 cases, with a 27% share of cases bought.

```{r}

hist(df$TARGET)

```

For this reason, a zero-inflated model may give us the best results.
```{r}



library(pscl)

model.zi = zeroinfl(TARGET ~ .,
                    data = df7a,
                    dist = "poisson")
summary(model.zi)


print(paste("AIC: ", AIC(model.zi)))

#dd <- tidy(model.zi)
#write.csv(dd, "C:\\Users\\eric.hirsch\\Desktop\\RStudio\\CUNY_621\\Assignment 5\\zeroinf1.csv")
```
In this model the AIC has fallen to 39841.  This is the best model so far, even better than our optimized OLS model.

We can use backward elimination to see if we can decrease our AIC.

```{r}

```


```{r}


#library(car)

#Anova(model.zi,
      #type="II",
      #test="Chisq")


#library(rcompanion)

#nagelkerke(model.zi)

#library(multcompView)
#library(multcomp)

#library(emmeans)

#marginal = emmeans(model.zi,
                  # ~ STARS)

#pairs(marginal,
      #adjust="tukey")

#cld(marginal,
    #alpha=0.05,
    #Letters=letters,  ### Use lower-case letters for .group
    #adjust="tukey")   ### Tukey adjustment for multiple comparisons




```


#### A. Interaction terms

As stated before, the dataset appears to hold the potential for many interaction terms, as many distributions suggest areas of very low industrialization and very high industrialization, which may affect the slope of other variables.  We create some dummy variables and look for interactions.  These are the dummy variables we've chosen, based on the histograms above:

    TaxOver600
    radOver10
    ptOver14
    lstatOver12
    IndusOver16
    ZnOver0
    NoxOverPoint8
    MedvBelow50

The following are just some of the possible interactions we discover affecting the dataset:

```{r}


```

From these plots, we might draw a number of conclusions.  First, in the most highly industrialized areas the crime rate appears to be high no matter the other factors. Zone 0 areas behave differently from zone 1 areas. In the few schools where the pt ratio is very small, crime rates are high, even though in general crime rates increase as ptratio increases.  There may be other conclusions we can draw as well.

These interactions may or may not prove useful.  We don't want to overfit the data, so it may be enough to simply include the dummy variables.

#### B. Transformations

When the predictor variable is normally distributed but with different variance for the two values of y, we may try a quadratic function of x.  rm may be considered a reasonable candidate for this.

For skewed distributions, we may include both x and log(x).  nox, age, dis and lstat may be candidates for this. 

Therefore we add the following transformations:

rmSquared, nox_log, age_log, dis_log, lstat_log


### 3. Build Models

#### A. Base Model

We begin with the base model (all original variables from the original dataset).  


```{r}


```

In the base model, only 4 of the 12 predictors are not significant. AIC is 218. Through backward elimination we remove rm, lstat, chas and indus and arrive at a model with an AIC of 215.

Now we split the dataset 80/20 and perform 100 training iterations to test model predictive power.  

```{r include=FALSE}

```

The base model alone is an excellent predictor of crime rate. Accuracy is 91%. AIC of the smaller model is 171 (AIC will drop with fewer observations.)

__*The final results for this model, averaging 100 rounds on an 80/20 split are 91% accuracy and an AIC of 215 for the full model.*__

#### B. Enhanced Model with Dummies and Transformations

Now we try a model with all of our dummy variables from above and as well as our interactions:

```{r}

  
```

```{r include=FALSE}


```

Despite the larger number of variables, many of which are not significant, our AIC improves substantially to 178.  Through backward elimination we remove a number of variables and reduce the AIC to 163.  The new model can be seen below. Despite the fact that the tax variable does not seem significant, we leave it in, as taking it out increases AIC substantially.

```{r}

```


We split the dataset 80/20 and perform 100 training iterations to test model predictive power.

```{r include=FALSE}


```
```{r}

```


The model does a better job predicting outcomes (accuracy = .94) than the base model, and the AIC for the smaller model falls from 171 to 134. 

__*The final results for this model, averaging 100 rounds on an 80/20 split are 94% accuracy and an AIC of 163 for the full model.*__

#### C. Enhanced Model with Interaction Terms

For this model we add interaction terms in addition to dummies and transformations.  We choose the following interactions as they seem the most promising:

inter_z_rm = ZnOver0$*$rm\
inter_age_indus = IndusOver16$*$age\
inter_rad_lstat = radOver10$*$lstat\
inter_pt_rad = ptOver14$*$rad

```{r}

```

```{r}



```
None of the interaction terms are significant. AIC for the larger model climbs to 185, and the for the smaller it rises as well. Accuracy falls below 93%.  We will therefore reject the use of interaction terms.


```{r include=FALSE}


```
```{r}


```


### 4. Select Model

We choose model B as the model with both best accuracy and lowest AIC. First we run some diagnostics on the model.

```{r}

```

Observations 338 and 280 appear to be outliers and possibly influential points (especially 280).  Since they are only two points, we will eliminate them to see their impact on accuracy and AIC:


```{r include=FALSE}


```
```{r}


```

This improves both accuracy and AIC and so (with the risk of overfitting) we accept the change.

Our final model is as follows. Results may differ since there as an 80/20 split - however, what is shown below is typical.

```{r}

```

The final step is to make predictions on the evaluation set:

```{r}

#makePredictions <- function(m)
#{
#predictions <- as.data.frame(predict(m,newdata=dfEval, type="response"))
#write_csv(predictions, "D:\\RStudio\\CUNY_621\\predictionsCrime.csv")
#}

#a <- makePredictions(m1)

#head(a)

```
### 5. Conclusion

We examined 466 records of town statistics to create a predictive model of whether crime rates were above the median or not. We used a logistic regression to do this, testing our models on an 80/20 split 100 times and taking the average accuracy and AIC.

Several enhancements to the model increased accuracy and lowered AIC.  First, some predictors were transformed with the log or square to improve fit. Second, dummy variables were introduced to capture the fact that highly industrial areas appeared to operate by a different logic than mixed use areas.  interaction terms to model this phenomenon did not improve the model.  The final model was 94% accurate on the holdout set.

