---
title: "605_Final_HomeSales"
author: "Eric Hirsch and Cameron Smith"
date: "12/13/2021"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

```{r}
# Load libraries
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(data.table)
library(tidymodels)
library(vip)
library(tidyverse)
library(lmtest)
library(skimr)
library(mltools)
library(psych)
library(MASS)
library(broom)
```

We are modeling a data set containing 1460 records of houses sold in the Ames, Iowa area between 2006 and 2010.  The variables are mostly related to house features, such as square footage, the presense of a pool, etc. The response variable, "SalePrice", is a continuous variable representing the sale price of the house in dollars.  

We examine the data:

```{r}

dfTrain <- read.csv("https://raw.githubusercontent.com/ericonsi/CUNY_621/main/Final/train.csv", stringsAsFactors=TRUE, header = TRUE)

dfTest <- read.csv("https://raw.githubusercontent.com/ericonsi/CUNY_621/main/Final/test.csv", stringsAsFactors=TRUE, header = TRUE)

```
### 1. Dataset Description

#### A.  Summary Statistics

```{r}
summary(dfTrain)
str(dfTrain)
```
The dataset consists of 1460 observations and 81 variables, some numeric and some categorical.  The target variable has a minimum of 34,950 and a maximum of 7,550,000. The low median compared to the mean suggests some skew.

#### B. Missing values

There are missing values scattered throughout the dataset.  We analyse them:

```{r}

dfMissing <- dfTrain %>%
  dplyr::select(which(colMeans(is.na(.)) >0))

dfMissing2 <- dfTrain[rowSums(is.na(dfTrain)) > 0, ]      

mm <- EHSummarize_MissingValues(dfMissing)

mm[[1]]
mm[[3]]

```

A few categorical features like fireplace, fence, etc. take up the bulk of missings.  They do not appear to be important enough to retain so we delete them (FireplaceQu, Fence, Alley, MiscFeature, PoolQC, and LotFrontage).  We impute the mean for the rest.  



```{r}

dfTrain1 <- dfTrain %>%
  dplyr::select(-FireplaceQu, -Fence, -Alley, -MiscFeature, -PoolQC, -LotFrontage)

dfTest1 <- dfTest %>%
  dplyr::select(-FireplaceQu, -Fence, -Alley, -MiscFeature, -PoolQC, -LotFrontage)

dfTrain2 <- EHPrepare_MissingValues_Imputation(dfTrain1)
dfTest2 <- EHPrepare_MissingValues_Imputation(dfTest1)

```


#### C. Create dummy variables

Now we create dummy variables for all of the character variables.  Categorical NA's will be handled by adding a dummy variable for NA.

```{r}

library(tidytable)

dfTrain3 <- EHPrepare_CreateDummies(dfTrain2)
dfTest3 <- EHPrepare_CreateDummies(dfTest2)

```

#### D. Reconcile training and test sets

We check if the dataset is missing columns from the test dataset and if so, drop them from the training set.  This way we don't risk making predictions on training set variables not found in the test set.

```{r}
g <- EHPrepare_RestrictDataFrameColumnsToThoseInCommon(dfTrain3, dfTest3, exclude=c("SalePrice"))

dfTrain4 <- g[[1]]
dfTest4  <- g[[2]]

```

#### E. Multicollinearity

We examine multicollinearity in the database  We look at all of the pairs of correlations over .8  There are 24 pairs.

```{r}

mult6 <- EHExplore_Multicollinearity(dfTrain4, printHighest=TRUE, threshold=.8, printHeatMap=FALSE)


```

Most of the pairs make sense - siding on the first floor will match siding on the sencond floor, the number of cars a garage can hold will be related to its area.  We will address the multicollinearity more closely when we run the analysis.

### 2. Transformations

#### A. Log of SalePrice

The skew in the dependent variable suggests a log transformation.

```{r}

dfTrain5 <- na.omit(dfTrain4)
library(caTools)
library(Metrics)
dfTrain5$SalePrice <- log(dfTrain5$SalePrice)

dfTest5 <- dfTest4
```

```{r}

#EHSummarize_StandardPlots(df6, "SalePrice")

```

#### B. Other transformations

A number of histograms suggest issues with some of the independent variables.

```{r}

dfLook <- dfTrain5 %>%
  dplyr::select(LotArea, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, X1stFlrSF, GrLivArea, GarageYrBlt, WoodDeckSF, OpenPorchSF)

EHSummarize_SingleColumn_Histograms(dfLook)

```



We can see some transformations might be useful.  We:
1. Add a dummy variable to mark YearBuilt before and after 1920
2. We set YearRemodAdd = 1950 to 0, and create a dummy variable YearRemodUnknown to track it
3. We add dummies for NoFinBsmt, HasDeck, and HasPorch
4. We eliminate outliers by setting LotArea<35000, GrLivArea3500 and BsmtFinSF1<4000

```{r}
dfTrain6 <- dfTrain5 %>%
  dplyr::mutate(BuiltAfter1920 = ifelse(YearBuilt>1920,1,0), YearRemodUnknown = ifelse(YearRemodAdd==1950,1,0), NoFinBsmt = ifelse(BsmtFinSF1==0,1,0), YearRemodAdd = ifelse(YearRemodAdd==1950,0,YearRemodAdd), HasDeck = ifelse(WoodDeckSF!=0,1,0), HasPorch = ifelse(OpenPorchSF!=0,1,0)) %>%
  dplyr::filter(LotArea<35000, GrLivArea<3500, BsmtFinSF1<4000)

dfTest6 <- dfTest5 %>%
  dplyr::mutate(BuiltAfter1920 = ifelse(YearBuilt>1920,1,0), YearRemodUnknown = ifelse(YearRemodAdd==1950,1,0), NoFinBsmt = ifelse(BsmtFinSF1==0,1,0), YearRemodAdd = ifelse(YearRemodAdd==1950,0,YearRemodAdd), HasDeck = ifelse(WoodDeckSF!=0,1,0), HasPorch = ifelse(OpenPorchSF!=0,1,0)) 
```

### 3. Model and Predict:

#### A. Base Model
We run a regression using the stepAIC algorithm to minimize AIC.

```{r}

abc <- EHModel_Regression_StandardLM(dfTrain6, "SalePrice", splitRatio = 1, returnLM=TRUE)


```


Now we make predictions
```{r}

makePredictions2 <- function(df)
{
predictions <- predict(df,newdata=dfTest6)
predictions <- data.frame(as.vector(predictions))
predictions$Id <- dfTest6$Id
predictions[,c(1,2)] <- predictions[,c(2,1)]
colnames(predictions) <- c("Id", "SalePrice")
predictions[is.na(predictions)] <- log(mean(dfTrain$SalePrice))
predictions$SalePrice <- exp(predictions$SalePrice)
#write_csv(predictions, "C:\\Users\\eric.hirsch\\Desktop\\predictionsABCLess.csv")
write_csv(predictions, "D:\\RStudio\\CUNY_621\\Final\\predictionsABC.csv")
}

makePredictions2(abc)

```
We achieve a score of .14586 on kaggle.

#### B. Now we try Ridge regression:

```{r}

library(glmnet)

df7a <- EHData::EHPrepare_ScaleAllButTarget(dfTrain6, "SalePrice")
dfSubmit7a <- as.data.frame(scale(dfTest6))

df7b <- df7a %>%
    dplyr::select(-SalePrice)

y <- df7$SalePrice
x <- data.matrix(df7b)
xSub <- data.matrix(dfSubmit7a)

model <- glmnet(x, y, alpha = 0)
```

R makes it easy to find the best lambda by using kfold validation:

```{r}


#We find the optimal lambda by performing k-fold cross validation:

mcv <- cv.glmnet(x, y, alpha = 0)
plot(mcv)

lambda1 <- mcv$lambda.min

plot(model, xvar = "lambda")

m10 <- glmnet(x, y, alpha = 0, lambda = lambda1)
coef(m10)

x2 <- tidy(coef(m10))

y_predicted <- predict(m10, s = lambda1, newx = xSub)
```

We predict values based on our Ridge regressions.  


```{r}

mSubmit7 = as.matrix(dfSubmit7)
mod=m10

predictions <- predict(mod, s = lambda1, newx = xSub)
predictions <- data.frame(as.vector(predictions))
predictions$Id <- dfSubmit7$Id
predictions[,c(1,2)] <- predictions[,c(2,1)]
colnames(predictions) <- c("Id", "SalePrice")
predictions[is.na(predictions)] <- log(mean(home_sales$SalePrice))
predictions$SalePrice <- exp(predictions$SalePrice)
write_csv(predictions, "D:\\RStudio\\CUNY_621\\Final\\predictionsRidge2.csv")

```

Ridge regression performs the best, with a score of .14047.  This puts us at 1690 out of 4216 individuals.
