---
title: "621_Final_HomeSales"
author: "Eric Hirsch, Cameron Smith and Carlisle Fergusen"
date: "4/7/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 4
header-includes:
    - \usepackage{setspace}\doublespacing
---


```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =  FALSE, message = FALSE)
```

```{r, include=FALSE}
# Load libraries
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(data.table)
library(tidymodels)
library(vip)
library(tidyverse)
library(lmtest)
library(skimr)
library(mltools)
library(psych)
library(MASS)
library(broom)
```

## *Abstract*

Being able to accurately predict housing prices is critical to many industries. Recently, analysts have attempted to improve price prediction with enhanced statistical techniques.  In this paper, we take a more comparative approach, examining 4 standard regression techniques (OLS, ridge lasso, and elastic net) to assess the best performance.  We used a kaggle dataset (https://www.kaggle.com/c/house-prices-advanced-regression-techniques) in order to test the performance of the model.  We found Lasso to be the best predictor, which we speculate is because the dataset has a high number of predictors relative to the number of observations. 

## *Introduction*

In this paper we analyze housing prices by comparing three prediction methodologies: OLS, Ridge regression, and Random Forest. The purpose is to compare the methodologies and draw conclusions about which are most effective and why. Regression alone is not necessarily the optimal strategy for predicting housing prices.^[1 Prediction of China's Housing Price Based on a Novel Grey Seasonal Model, Li et al.  Mathematical Models in Engineering, 
https://www.hindawi.com/journals/mpe/2021/5541233/]  However, when data sets and/or analysis resources are limited, regression can perform adequately.

## *Background and Literature Review*

The ability to accurately predict home prices is of tremendous value to a number of industries, including investors, real estate agents, and municipalities who depend upon property tax revenue. ¹ Predictive models for home prices fall roughly into two kinds.  First, there are those which predict market trends, busts, and booms.  These predictions rely mainly on timeseries data and analysis of housing prices in the aggregate.  The other type of prediction involves the capacity to predict individual house prices from a set of factors.  These usually employ some form of regression and/or machine learning.^[2]

For either sort of prediction, there is no consensus about the best method.  Many researchers have sought to enhance the traditional models with other methodologies.^[3]  For example, Guan et. al. propose a “data stream” approach in which past sale records are treated as an evolving datastream.^[4]   Li et. al. introduce a “grey seasonal model” in which seasonal fluctuations are modeled using grey systems theory, which incorporates uncertainty.^[5]  Alfiyatin, et. el. use particle swarm optimization (PSO) to select independent variables.^[6]  (PSO is an optimization system in which population is initialized with random solutions and searches for optima by updating generations.)  Finally, Liu et.al incorporate both spatial and temporal autocorrelation in their models by analyzing experience-based submarkets by real estate professionals.^[7]

All of these researchers report that their innovations improve their regression models. Indeed, any real estate agent can tell you that a predictive model can be improved simply by knowing what other houses in the neighborhood sold for.  The problem is, the data at the center of these enhancements is not always available. The researcher may have home sales from only a short time span, and neighborhoods that are not defined by real estate experts but by traditional boundary lines which may contain a mix of house types.  Even when data is available, the complex models proposed may be computationally expensive and/or require data analysis expertise that is not generally available.

In this project we approach the question comparatively. Restricting ourselves to regression models, we compare three types of regression: OLS, Ridge, and Random Forest. At the data is drawn from the Advanced Regression Techniques housing data set for Ames, Iowa. We test the accuracy of our models by submitting each to the Kaggle competition to see how they perform.  We then discussed the merits of the different sorts of approaches.


## *Modeling*

We are modeling a data set containing 1460 records of houses sold in the Ames, Iowa area between 2006 and 2010.  The variables are mostly related to house features, such as square footage, the presense of a pool, etc. The response variable, "SalePrice", is a continuous variable representing the sale price of the house in dollars.  

We examine the data:

```{r}

dfTrain <- read.csv("https://raw.githubusercontent.com/ericonsi/CUNY_621/main/Final/train.csv", stringsAsFactors=TRUE, header = TRUE)

dfTest <- read.csv("https://raw.githubusercontent.com/ericonsi/CUNY_621/main/Final/test.csv", stringsAsFactors=TRUE, header = TRUE)

```
### 1. Dataset Description

#### A.  Summary Statistics

```{r}
summary(dfTrain)
str(dfTrain)
```
The dataset consists of 1460 observations and 81 variables, some numeric and some categorical.  The target variable has a minimum of 34,950 and a maximum of 7,550,000. The low median compared to the mean suggests some skew.

#### B. Missing values

There are missing values scattered throughout the dataset.  We analyse them:

```{r}

dfMissing <- dfTrain %>%
  dplyr::select(which(colMeans(is.na(.)) >0))

dfMissing2 <- dfTrain[rowSums(is.na(dfTrain)) > 0, ]      

mm <- EHSummarize_MissingValues(dfMissing)

mm[[1]]
mm[[3]]

```

A few categorical features like fireplace, fence, etc. take up the bulk of missings.  They do not appear to be important enough to retain so we delete them (FireplaceQu, Fence, Alley, MiscFeature, PoolQC, and LotFrontage).  We impute the mean for the rest.  



```{r}

dfTrain1 <- dfTrain %>%
  dplyr::select(-FireplaceQu, -Fence, -Alley, -MiscFeature, -PoolQC, -LotFrontage)

dfTest1 <- dfTest %>%
  dplyr::select(-FireplaceQu, -Fence, -Alley, -MiscFeature, -PoolQC, -LotFrontage)

dfTrain2 <- EHPrepare_MissingValues_Imputation(dfTrain1)
dfTest2 <- EHPrepare_MissingValues_Imputation(dfTest1)

```


#### C. Create dummy variables

Now we create dummy variables for all of the character variables.  Categorical NA's will be handled by adding a dummy variable for NA.

```{r}

library(tidytable)

dfTrain3 <- EHPrepare_CreateDummies(dfTrain2)
dfTest3 <- EHPrepare_CreateDummies(dfTest2)

```

#### D. Reconcile training and test sets

We check if the dataset is missing columns from the test dataset and if so, drop them from the training set.  This way we don't risk making predictions on training set variables not found in the test set.

```{r}
g <- EHPrepare_RestrictDataFrameColumnsToThoseInCommon(dfTrain3, dfTest3, exclude=c("SalePrice"))

dfTrain4 <- g[[1]]
dfTest4  <- g[[2]]

```

#### E. Multicollinearity

We examine multicollinearity in the dataset.  We look at all of the pairs of correlations over .8  There are 24 pairs.

```{r}

mult6 <- EHExplore_Multicollinearity(dfTrain4, printHighest=TRUE, threshold=.8, printHeatMap=FALSE)


```

Most of the pairs make sense - siding on the first floor will match siding on the sencond floor, the number of cars a garage can hold will be related to its area.  We will address the multicollinearity more closely when we run the analysis.

### 2. Transformations

#### A. Log of SalePrice

The skew in the dependent variable suggests a log transformation.

```{r}

dfTrain5 <- na.omit(dfTrain4)
library(caTools)
library(Metrics)
dfTrain5$SalePrice <- log(dfTrain5$SalePrice)

dfTest5 <- dfTest4
```

```{r}

#EHSummarize_StandardPlots(df6, "SalePrice")

```

#### B. Other transformations

A number of histograms suggest issues with some of the independent variables.

```{r}

dfLook <- dfTrain5 %>%
  dplyr::select(LotArea, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, X1stFlrSF, GrLivArea, GarageYrBlt, WoodDeckSF, OpenPorchSF)

EHSummarize_SingleColumn_Histograms(dfLook)

```


We can see some transformations might be useful.  We:
1. Add a dummy variable to mark YearBuilt before and after 1920
2. We set YearRemodAdd = 1950 to 0, and create a dummy variable YearRemodUnknown to track it
3. We add dummies for NoFinBsmt, HasDeck, and HasPorch
4. We eliminate outliers by setting LotArea<35000, GrLivArea3500 and BsmtFinSF1<4000

```{r}
dfTrain6 <- dfTrain5 %>%
  dplyr::mutate(BuiltAfter1920 = ifelse(YearBuilt>1920,1,0), YearRemodUnknown = ifelse(YearRemodAdd==1950,1,0), NoFinBsmt = ifelse(BsmtFinSF1==0,1,0), YearRemodAdd = ifelse(YearRemodAdd==1950,0,YearRemodAdd), HasDeck = ifelse(WoodDeckSF!=0,1,0), HasPorch = ifelse(OpenPorchSF!=0,1,0)) %>%
  dplyr::filter(LotArea<35000, GrLivArea<3500, BsmtFinSF1<4000)

dfTest6 <- dfTest5 %>%
  dplyr::mutate(BuiltAfter1920 = ifelse(YearBuilt>1920,1,0), YearRemodUnknown = ifelse(YearRemodAdd==1950,1,0), NoFinBsmt = ifelse(BsmtFinSF1==0,1,0), YearRemodAdd = ifelse(YearRemodAdd==1950,0,YearRemodAdd), HasDeck = ifelse(WoodDeckSF!=0,1,0), HasPorch = ifelse(OpenPorchSF!=0,1,0)) 
```

### 3. Model and Predict:

#### A. Base Model
We run a regression using the stepAIC algorithm to minimize AIC.

```{r}

#abc <- EHModel_Regression_StandardLM(dfTrain6, "SalePrice", splitRatio = 1, returnLM=TRUE)
abc <- lm(SalePrice ~ GrLivArea, dfTrain6)
summary(abc)

```


Now we make predictions
```{r}

makePredictions2 <- function(df)
{
predictions <- predict(df,newdata=dfTest6)
predictions <- data.frame(as.vector(predictions))
predictions$Id <- dfTest6$Id
predictions[,c(1,2)] <- predictions[,c(2,1)]
colnames(predictions) <- c("Id", "SalePrice")
predictions[is.na(predictions)] <- log(mean(dfTrain$SalePrice))
predictions$SalePrice <- exp(predictions$SalePrice)
#write_csv(predictions, "C:\\Users\\eric.hirsch\\Desktop\\predictionsABCLess.csv")
write_csv(predictions, "D:\\RStudio\\CUNY_621\\Final\\predictionsABC.csv")
}

makePredictions2(abc)

```
We achieve a score of .14586 on kaggle.

#### B. Now we try Ridge regression:

```{r}

library(glmnet)

df7a <- EHData::EHPrepare_ScaleAllButTarget(dfTrain6, "SalePrice")
dfSubmit7a <- as.data.frame(scale(dfTest6))

df7b <- df7a %>%
    dplyr::select(-SalePrice)

y <- df7a$SalePrice
x <- data.matrix(df7b)
xSub <- data.matrix(dfSubmit7a)

model <- glmnet(x, y, alpha = 0)
```

R makes it easy to find the best lambda by using kfold validation:

```{r}


#We find the optimal lambda by performing k-fold cross validation:

mcv <- cv.glmnet(x, y, alpha = 0)
plot(mcv)

lambda1 <- mcv$lambda.min

plot(model, xvar = "lambda")

m10 <- glmnet(x, y, alpha = 0, lambda = lambda1)
coef(m10)

x2 <- tidy(coef(m10))

y_predicted <- predict(m10, s = lambda1, newx = xSub)
```

We predict values based on our Ridge regressions.  


```{r}

mSubmit7 = as.matrix(dfSubmit7a)
mod=m10

predictions <- predict(mod, s = lambda1, newx = xSub)
predictions <- data.frame(as.vector(predictions))
predictions$Id <- dfTest6$Id
predictions[,c(1,2)] <- predictions[,c(2,1)]
colnames(predictions) <- c("Id", "SalePrice")
predictions[is.na(predictions)] <- log(mean(dfTrain$SalePrice))
predictions$SalePrice <- exp(predictions$SalePrice)
write_csv(predictions, "D:\\RStudio\\CUNY_621\\Final\\predictionsRidge2.csv")

```

Ridge regression performs the best, with a score of .14047.  This puts us at 1690 out of 4216 individuals.

#### C. Lasso Regression

##### Lasso Regression with Unscaled Data

First we define the predictor and response variables for the training dataset.
```{r}
y_train <- dfTrain6$SalePrice
x_train <- data.matrix(dfTrain6[, names(dfTrain6) != "SalePrice"])
```

Similarly to the Ridge model, we'll use the `glmnet` library, which makes it easy to use k-fold cross-validation to find the optimal value for lambda.
```{r, include=FALSE}
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1)

lasso_lambda <- lasso_cv$lambda.min; lasso_lambda
```
Next, we find the coefficients for the Lasso model using our optimized lambda.

```{r, include=FALSE}
optimized_lasso <- glmnet(x_train, y_train, alpha = 1, lambda = lasso_lambda)
```

Lastly, we predict new values using our optimized Lasso model.

```{r, include=FALSE}
lasso_testdata <- data.matrix(dfTest6)
```

```{r, include=FALSE}
y_pred_lasso <- predict(optimized_lasso, s = lasso_lambda, newx = lasso_testdata)
y_pred_lasso <- data.frame(as.vector(y_pred_lasso))
y_pred_lasso$Id <- dfTest6$Id
y_pred_lasso[,c(1,2)] <- y_pred_lasso[,c(2,1)]
colnames(y_pred_lasso) <- c("Id", "SalePrice")
#y_pred_lasso[is.na(y_pred_lasso)] <- log(mean(home_sales$SalePrice))
y_pred_lasso$SalePrice <- exp(y_pred_lasso$SalePrice)
```
```{r}
#write.csv(y_pred_lasso, "C:\\Users\\carli\\OneDrive\\Documents\\621\\LassoPredictions.csv")
```

##### Lasso Regression with Scaled Data

```{r}
scaled_training <- data.matrix(df7b)
```


```{r}
scaled_lasso <- cv.glmnet(scaled_training, y_train, alpha = 1)

```
```{r}
lasso_scaledlambda <- scaled_lasso$lambda.min; lasso_scaledlambda
optimizedscaled_lasso <- glmnet(scaled_training, y_train, alpha = 1, lambda = lasso_scaledlambda)


```
```{r}
xSub <- data.matrix(dfSubmit7a)
```
```{r}
y_pred_lasso2 <- predict(optimizedscaled_lasso, s = lasso_scaledlambda, newx = xSub)
y_pred_lasso2 <- data.frame(as.vector(y_pred_lasso2))
y_pred_lasso2$Id <- dfTest6$Id
y_pred_lasso2[,c(1,2)] <- y_pred_lasso2[,c(2,1)]
colnames(y_pred_lasso2) <- c("Id", "SalePrice")
#y_pred_lasso[is.na(y_pred_lasso)] <- log(mean(home_sales$SalePrice))
y_pred_lasso2$SalePrice <- exp(y_pred_lasso2$SalePrice)
```
```{r}
#write.csv(y_pred_lasso2, "C:\\Users\\carli\\OneDrive\\Documents\\621\\LassoPredictions2.csv")
```

Our lasso regression gives us a .1375, which outperforms ridge.

#### D. Elastic Net Regression
```{r, include=FALSE}
library(caret)
library(dplyr)
```
First, build a control model.
```{r, include=FALSE}
control <- trainControl(method = "repeatedcv", number = 5, repeats = 5, search = "random", verboseIter = TRUE)
```
Next, train the elastic net regression model.
```{r, include=FALSE}
enet <- train(SalePrice ~ ., data = dfTrain6, method = "glmnet", preProcess = c("center", "scale"), tuneLength = 25, trControl = control)
```
Optimizing the elastic net model based on tuning parameters selected from model training.
```{r, include=FALSE}
optimized_enet <- glmnet(scaled_training, y_train, alpha = .162, lambda = 0.0289)
```

```{r, include=FALSE}
y_pred_enet <- predict(optimized_enet, xSub)
y_pred_enet <- data.frame(as.vector(y_pred_enet))
y_pred_enet$Id <- dfTest6$Id
y_pred_enet[,c(1,2)] <- y_pred_enet[,c(2,1)]
colnames(y_pred_enet) <- c("Id", "SalePrice")
#y_pred_lasso[is.na(y_pred_lasso)] <- log(mean(home_sales$SalePrice))
y_pred_enet$SalePrice <- exp(y_pred_enet$SalePrice)
```
```{r}
#write.csv(y_pred_enet, "C:\\Users\\carli\\OneDrive\\Documents\\621\\ElasticNetPredictions.csv")
```

Our elastic net result falls between ridge and lasso.

## *Discussion and Conclusions*

Ordinary Least Squares is a regression technique with a long history of use as a predictive model. However, standard measures of fit (like R^2) will always increase (or stay the same) as you add independent variables.  This can result in models which incorporate noise - in other words, overfit the data so that idiosyncrasies in the training set affect predictions in the test set.  Other methods of measuring fit, such as adjusted R^2 and AIC, help mitigate the overfitting effect by penalizing the addition of factors.

More recently, other techniques which employ regularization have been introduced to deal with overfit.  For example, in ridge regression, we reduce the sum of our coefficients, not the number of variables.  We do this by introducing a penalty in the loss function represented by the squared sum of the coefficients themselves, multiplied by a factor (designated as lambda) which allows us to control the degree to which the size of the coefficients matters. If lambda is zero, there is no difference between ridge regression and OLS.

Ridge regression will keep all the variables but may significantly reduce the coefficients for some.  Lasso regression is similar in that it employs a constraint where the sum of the absolute value of the coefficients is less than a fixed value.  Lasso regression may drop coefficients altogether to stay under the constraint.

Elastic Net regression is a hybrid approach that blends both of the penalizations of lasso and ridge methods.  An alpha parameter weights which penalty to emphasize - lasso or ridge.

Our dataset has features that lend to overfitting. Most significant of these is the high number of potential independent variables (over 200 once the dummy variables are created.)  Multicollinearity is also a problem, though less than we might have expected. 

We used stepAIC to fit our OLS model.  StepAIC uses backward substitution to find the best model with the lowest AIC.  With an adjusted R^2 of over 90% overfitting was expected.  However, even with an overfit model our predictions performed at the 60th percentile on the Kaggle.

Because of the large number of potential predictors, ridge (and by extension elastic net) were not as good candidates as Lasso - however, potential issues with collinearity actually favored ridge.  We found that Lasso improved our score the most, followed be elastic net (which is a compromise between lasso and ridge), followed by ridge.  All were improvements over OLS - however, the improvements were not dramatic.    

In conclusion, it is important to keep in mind that while regularization improved our model, the base OLS model also performed adequately, so regularization, while important, may in some cases improve models at the margin.  It is also important to recognize the strengths of each of the techniques and use the appropriate one for the situation.


